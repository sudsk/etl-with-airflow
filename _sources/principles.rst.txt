Principles
==================================================

.. important::

    What you will find here are data engineering principles...
	
	
DB1-Primary Keys 
-------------------------------------------------------------

Keys should be unique, meaningless and unchanging

These are considered to be good principles for keys as it helps you avoid issues around using common data items like email addresses as the basis of some sort of key or identifier. 

Product codes are another common trap that are sometimes used as codes - which is fine until the product code changes

DB2-Access via DataProc
-------------------------------------------------------------

DB2 Access to databases should be via stored procedures, but logic should only be added to preserve integrity or another NFR property.

Direct table access should be avoided. Data extraction methods e.g. views, functions, stored procedures do provide an additional layer of security as the set of permitted operations on the database are constrained by removing direct table access permissions. This also provides an abstraction layer that can enable database structure changes to be abstracted out and thus isolate the change impact. Logic inside stored procedures is encouraged if it helps protect data integrity, security or performance and be a data logic. Information logic should be left for the application to handle.

DB3-Change control tools

We should make use of database change control tools to ease the maintenance and deployment of database changes

The code to create/modify database structures and/or content must be controlled with the same rigour and comparable tools as that for those used for software applications. This also facilitates consistent test results across environments.

DB4-Batch progress
-------------------------------------------------------------
Batch jobs should demonstrate linear progress with time by committing in modest batch sizes. 

Batch jobs that are long running and either totally commit or totally roll-back are forbidden unless that behaviour is specifically required.

Bulk inserts into a database within a transactional scope tend to be more performant for a limited batch sizes within the range of 500-1000 rows. Extending the number of inserts beyond this size has two detrimental effects. Firstly the transaction rollback log file will grow very large and second progress becomes opaque. If you kill a job after 60 hours you lose all progress but yet the job might have fully completed after 5 more minutes.

AN ETL 'job' (control-m?) should be transparent and broken down into Steps & Tasks (workflow).
Job → Steps → Tasks
The workflow should be defined, so that the Steps and Tasks are transparent and visible, and during execution it should be monitor-able to see progress made on Steps and Tasks. 
Steps could be parallel or serial. [MDJ to reword]

DB5-Batch restartable & idempotent
-------------------------------------------------------------
Batch jobs must be designed to be automatically restartable from the point where they left off and be idempotent.

Running the same batch job multiple times should only increase progress of the job until completion, it should not introduce any side-effects in terms of duplicated record creation. Running the same job on a data set already processed should have no side-effect

When you run ETL to load data from a source system into the DW, it is possible that you may need to restart the ETL load after a failure. After such a failure during ETL, to avoid restarting the entire ETL after a failure, which would require inefficient re-runs of all ETL tasks, you must restart the ETL from the same point in its execution once the cause of failure has been diagnosed and resolved.
Restartability Grain - When you restart an ETL after a failure, you may not restart again from the exact point of failure, depending on where it occurred and on dependencies between ETL steps. The point of restartability is that the end result of the ETL execution is the same regardless of any ETL failure.
To maintain data integrity in the case of restart, the grain would vary depending on the location in the hierarchy of the failed step and on the Restart setting for the step in the workflow definition. Basically meaning the restartability grain should be definable at a step level. [MDJ to reword]

DB6-Batch monitorable
-------------------------------------------------------------
The progress of all batch jobs including progress, completion and failure must be explicitly observable by operations teams

It is vital that operations teams can monitor the observable progress of all long running batch jobs and the final outcome of the job.
The workflow should support monitoring Steps/Tasks completed, running and pending. All jobs must adhere to incremental progress in reasonable measures.

DB7-Batch terminability
-------------------------------------------------------------
All batch jobs must be killable with no side-effect by the operations team, explicit automated kill commands must be provided as part of the batch job / script.

There can be no data or integrity loss as a result of killing a batch job by using the specified kill command

DB8-Common data lexicon
-------------------------------------------------------------
Seek a common data lexicon to build your domain model.

Mushrooming words for the same items are only recipes for confusion, management nightmares.

DB9-(Re)-buildable
-------------------------------------------------------------
Our databases should be (re)-buildable from an automated, version controlled source, both schema and data

We need reliable, repeatable process for this, so that we remove it as a potential bottleneck / problem on the speed of our delivery teams

DB10-Business logic
-------------------------------------------------------------
The database is not to be concerned by business logic.

The database job is about data. The business context that turns it into 'information' is the application concern. Focus on each strength. The activities of the database should be focussed on the handling and manipulation of data and stay away from external services.

DB11-Set vs Row
-------------------------------------------------------------
Always process at the set level, avoid processing individual rows.

In todays world, parsing large data volumes through cursors will prove impossibly challenging and beyond under performing. Bulk methods are available for all technologies.

DB12-Think performance
-------------------------------------------------------------
Think performance before writing first line of code.

Performance should be a Day-1 concern of writing code, not an afterthought, reactive and performance tuning activity
Think performance on Day-0. Think performance before data modelling. Think performance before writing first line of code. 
Define Your Performance Goals from the Start.
Test Against Representative Data - Don't wait till performance issues manifest in PPE and PROD.

DB13-Unit performance testing
-------------------------------------------------------------
As a part of unit testing, performance should be tested and recorded as well, wherever possible. Shouldn't be left until the code reaches a performance test environment (e.g. Pre-Prod).

DB14-Code Instrumentation
-------------------------------------------------------------

The workflow should enable logs capture, which then makes analytics available for Steps and Tasks such as last 3 months min, max and avg timings. All jobs must have some ability to produce a log file output for monitoring purposes

Enable analytics for how the application has performed in the past e.g. last 3 months e.g. for Oracle Logging, DBMS_APPLICATION_INFO

DB15-DB Black Box Syndrome
-------------------------------------------------------------
Avoid the DB Black Box Syndrome - Database Independence versus Database Dependence.
Make use of database native features instead of reinventing it outside. Native features are built to give the best performance. An example could be all types of native data-transfer mechanism (TTS, DBlink, Data-Pump) vs. generic JDBC.

DB16-It’s a Database, Not a Data Dump
-------------------------------------------------------------
However structured or unstructured the data repository is, it is vital that the content integrity is respected. The data acquired must be (made) workable.

DB17-Benchmark, Benchmark, Benchmark
-------------------------------------------------------------
Benchmark everything - all approaches, alternatives, current vs past performance.

DB18-Minimise database accesses
-------------------------------------------------------------
Try to minimise the total number of database accesses made by the application; especially if working on the WPG.

Performance of Shopper Cells at HVEs is restricted by database access. Cache data if you can.
