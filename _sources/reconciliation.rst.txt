Reconciliation
==================================================

.. important::

    Reconciliation increases the integrity levels of information stored within the data store/warehouse. 
	What you will find here are data reconciliation techniques and framework.

Principles
-------------------------
* Reconcile and validate information before it enters the data warehouse
* Validate information before, during, and after Extract, Transform, and Load (ETL) processes
* Verify and reconcile information already in your data warehouse against source systems
* Ensure that information is not lost or erroneously duplicated
* Reconcile report contents against the warehouse
* Perform reasonableness tests to detect potential problems
* Compute statistical tests to feed other controls or determine if expected thresholds are exceeded

Master Data Reconciliation
----------------------------

Reconcile the number of merchants in full load. 

* Source - Number of NAP Merchants - Active, Closed, Suspended 
* Target - Number of NAP Digital onboarded + non-onboarded - Active, Closed, Suspended 
* Source - Number of PAP Merchants -  Active, Closed, Suspended 
* Target - Number of PAP Digital onboarded + non-onboarded - Active, Closed, Suspended 

Reference Data Reconciliation
-------------------------------

Reconcile Currency, MCC, Card Product Type, etc.

			   
Transaction Data Reconciliation
--------------------------------

Reconcile count, balances e.g. sum, etc. from staging till the target layer and capture all business failures, exclusions count.


Source data transfer controls
------------------------------

Data transfer through tables
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For data transfer through tables, use the below structure:

.. csv-table:: Table ETL_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","TABLE_FILE_NAME","ETL_BATCH_ID","ROW_COUNT","BALANCE_SUM","BALANCE_COLUMN","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF403","STG_CQPR","10001","1996500","672945600","PAYER_AMOUNT","SYSDATE","ODI-<session-id>","",""
   "NAP","IF402","STG_MERCHANT_DETAILS","1021","8500","","","SYSDATE","ODI-<session-id>","",""
   
The control record is to be inserted post successful data transfer to the staging table e.g. once data is transferred and committed to STG_CQPR, then 
a record is inserted as shown above into the ETL_SOURCE_CONTROL table. This signifies that the transfer was successful and provides the counts and balances for reconciliation.
The context should exactly match as the staging table populated i.e. SOURCE_SYSTEM, INTERFACE_NAME, TABLE_FILE_NAME, and ETL_BATCH_ID should provide the
necessary context to tie the control record to the staging table. 

Fields Description:

* SOURCE_SYSTEM - NAP, PAP, etc.
* INTERFACE_NAME - IF402, IF403, IF414, IF408, etc.
* TABLE_FILE_NAME - Staging table names or file names e.g. STG_CQPR
* ETL_BATCH_ID - same as the one inserted into the staging table for this particular transfer
* ROW_COUNT - count of records in the staging table or file
* BALANCE_COLUMN - (optional field) the field used for balances e.g. PAYER_AMOUNT
* BALANCE_SUM - (optional field) the balance amount, usually the aggregate function should be SUM in all cases
* CREATED_DATE - (audit field) date of creation of this control record
* CREATED_BY - (audit field) process/session-id etc. of the process which inserted this control record
* LAST_UPDATED_DATE - (audit field) left blank as we don't expect this table to be updated
* LAST_UPDATED_BY - (audit field) left blank as we don't expect this table to be updated

Data transfer through files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For data transfer through files, the source should ideally provide a compressed data file and a control file post-completion of the transfer process.
This process is called "Token aware" - Tokens are files created in the file system to trigger an ETL event. Applications that are token aware can poll a directory for the arrival of a token file. 
The control file holds metadata information about the file transfers. The control file should have an ".info" extension.
There are variety of ways the data file and control file could be related - like one data file and one control file, multi data files and one control file, or multi data files and multi control files.

Take the example of simplest scenario - one compressed data file and one control file. The name of control file should be <data file name>.info.

The structure of control file is comma-delimited::

   $ cat test_20180425105233.info
   
   "FILE_NAME","FILE_COUNT","FILE_SIZE","CHECKSUM","CHECKSUM_TYPE"
   "test_20180425105233.gz","20000","43856377","1446861148","CKSUM"

The above control file shows that "test_20180425105233.gz" contains 20,000 files, the file size of "test_20180425105233.gz" is 43856377 bytes and CKSUM checksum is 1446861148.

Fields Description:

* FILE_NAME - data file name. could be a compressed file. if compressed use standard compression techniques e.g. gz or zip. 
* FILE_COUNT - if compressed file, then the number of files inside. if uncompressed file, then the number of records inside.
* FILE_SIZE - size of the file in bytes given by FILE_NAME
* CHECKSUM - checksum of the file given by FILE_NAME
* CHECKSUM_TYPE - checksum method e.g. CKSUM (for UNIX cksum) or MD5

The data file names should be unique, both the compressed file and it's content files should be unique. This is to prevent duplicates.
Duplicate files are a common occurrence when files are being received daily/weekly/monthly from another application. 
Sometimes, the previous day’s file has been sent in error. Prior to processing the file, business rules can be executed to ensure the 
correct day’s file will be processed, or in the case of an incorrect file being received, an exception report will be immediately flagged and 
the process will prevent the incorrect file from processing.

On the target side, the control-file should be loaded into the "ETL_OWNER.ETL_SOURCE_CONTROL" table:

.. csv-table:: Table ETL_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","TABLE_FILE_NAME","ETL_BATCH_ID","ROW_COUNT","SUM_AMOUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF414","test_20180425105233.gz","105","20000","","SYSDATE","ETL Process ID","",""

Streaming Data transfer
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Kafka Topics, JMS etc.

Reconciliation Framework
---------------------------
Combination of ETL tools capabilities and scripting techniques to get counts and balances at the source level when the application 
is providing the data extract. ETL tools are used to read the feed information and track them by updating and loading metadata tables into data warehouse. 
At the end of the process, reconciliation reports are generated to ensure transparency around the data ingestion process in addition to 
transformations that occur during the data movement. 
   
**Counts and Balances** - An example of the value of balancing information is when a program is used to extract data from a source system as input into a target system. 
Business rules are used to balance information such as record counts and sum of amounts on the source file and match the record counts and sum of amounts extracted for input into the target system.

**"RECON_CONTROL"** - captures reconciliation during ETL. Reconciliation is done between any two tables - a source and a target. 
A reconciliation task (DataProc) is created and which is called to make an entry in reconciliation tables post loading of target table.

.. csv-table:: Table ETL_OWNER.RECON_CONTROL
   :header: "RECON_ID","RECON_FROM","RECON_TO","ETL_BATCH_ID","SOURCE_ID","TARGET_ID","SOURCE_COUNT","TARGET_ELIGIBLE_COUNT","TARGET_LOADED_COUNT","TARGET_EXCLUDED_COUNT","TARGET_FAILURE_COUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","STG_CQPR","FINANCIAL_TRANS_EVENT","ETL_BATCH_ID",Source process id (if any),Target process id,1000,700,"SYSDATE","PROCESS_ID","",""

Fields Description:

* RECON_ID - sequence generated surrogate key
* RECON_FROM - source table
* RECON_TO - target table
* ETL_BATCH_ID - ETL_BATCH_ID from source table which is being processed/loaded to target
* SOURCE_ID - identifier for the source table if any such as process_id 
* TARGET_ID - identifier for the target table such as process_id 
* SOURCE_COUNT - count of records in source table
* TARGET_ELIGIBLE_COUNT - count of records eligible for loading into target e.g. if the source is split into two target loading - SME and Corporates.
* TARGET_LOADED_COUNT - Actual number of rows loaded to target
* TARGET_EXCLUDED_COUNT - Records eligible but not loaded into target because of exclusion filters or joins
* TARGET_FAILURE_COUNT - Records eligible but not loaded into target because of failing business constraints
* CREATED_DATE - (audit field) date of creation of this control record
* CREATED_BY - (audit field) process/session-id etc. of the process which inserted this control record
* LAST_UPDATED_DATE - (audit field) left blank as we don't expect this table to be updated
* LAST_UPDATED_BY - (audit field) left blank as we don't expect this table to be updated

**RECON_CONTROL_DETAILS** - captures breakup of exclusions and business failures from reconciliation.
   
.. csv-table:: Table ETL_OWNER.RECON_CONTROL_DETAILS
   :header: "RECON_ID","RECON_TYPE","KEY","VALUE","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","COUNT","101",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","102",176,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","103",32,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","105",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","106",78,"SYSDATE","PROCESS_ID","",""

Fields Description:

* RECON_ID - FK to RECON_CONTROL table
* RECON_TYPE - COUNT or BALANCE. should be stored in REF_CODE table. 
* KEY - FK to RECON_CONTROL_MASTER
* VALUE - stores count or balance amount
* CREATED_DATE - (audit field) date of creation of this control record
* CREATED_BY - (audit field) process/session-id etc. of the process which inserted this control record
* LAST_UPDATED_DATE - (audit field) left blank as we don't expect this table to be updated
* LAST_UPDATED_BY - (audit field) left blank as we don't expect this table to be updated
   
**RECON_CONTROL_MASTER** - captures master data of exclusions and business failures and actions to be taken.
   
.. csv-table:: Table ETL_OWNER.RECON_CONTROL_MASTER
   :header: "KEY","NAME","CATEGORY","DESCRIPTION","NOTIFICATION","REPROCESS_FLAG","RECOVERY_PROCESS","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto 
   
   "101","REPRESENTMENT","EXCLUSION","Representments","","N","","SYSDATE","PROCESS_ID","",""
   "101","NON-ONBOARDED","EXCLUSION","Merchants not onboarded","","N","","SYSDATE","PROCESS_ID","",""
   "101","CORRECTS","EXCLUSION","Corrects event","","N","","SYSDATE","PROCESS_ID","",""
   "101","PAN-MISSING","FAILURE","PAN is missing","","N","","SYSDATE","PROCESS_ID","",""
   "101","CURRENCY-MISSING","FAILURE","Currency is missing","","N","","SYSDATE","PROCESS_ID","",""

Fields Description:

* KEY - Sequence generated surrogate key
* NAME - Name of the exclusion or failure
* CATEGORY - one of - EXCLUSION, FAILURE or WARNING. WARNINGs are not part of reconciliation as they don't result in dropping of records e.g. MCC is blank.
* DESCRIPTION - description of the exclusion or failure 
* NOTIFICATION - Whether to send notifications and to whom
* REPROCESS_FLAG - Y or N - whether to attempt reprocessing at a later time.
* RECOVERY_PROCESS - Link recovery procedure if any
* CREATED_DATE - (audit field) date of creation of this control record
* CREATED_BY - (audit field) process/session-id etc. of the process which inserted this control record
* LAST_UPDATED_DATE - (audit field) left blank as we don't expect this table to be updated
* LAST_UPDATED_BY - (audit field) left blank as we don't expect this table to be updated
   
