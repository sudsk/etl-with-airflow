Reconciliation
==================================================

.. important::

    Reconciliation increases the integrity levels of information stored within the data store. 
	What you will find here are data reconciliation techniques and framework.

Principles
-------------------------
* Reconcile and validate information before it enters the data warehouse
* Validate information before, during, and after Extract, Translate, and Load (ETL) processes
* Verify and reconcile information already in your data warehouse against source systems and data marts
* Validate, balance, and reconcile sources to warehouse, data marts to warehouse, and data marts to sources
* Ensure that information is not lost or erroneously duplicated
* Reconcile report contents against the warehouse
* Perform reasonableness tests to detect potential problems
* Compute statistical tests to feed other controls or determine if expected thresholds are exceeded

Master Data Reconciliation
----------------------------

Reconcile the number of merchants in full load. 

* Source - Number of NAP Merchants - Active, Closed, Suspended 
* Target - Number of NAP Digital onboarded + non-onboarded - Active, Closed, Suspended 
* Source - Number of PAP Merchants -  Active, Closed, Suspended 
* Target - Number of PAP Digital onboarded + non-onboarded - Active, Closed, Suspended 

Reference Data Reconciliation
-------------------------------

Reconcile Currency, MCC, Card Product Type, etc.

			   
Transaction Data Reconciliation
--------------------------------

Reconcile count, sum, etc. from staging till the target layer and capture all business failures, exclusions count.


Source data  transfer metadata
------------------------
**Data transfer through tables**

For data transfer through tables, use the below structure:

.. csv-table:: Table REC_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","ETL_BATCH_ID","ROWCOUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF403-CQPR","10001","1996500","SYSDATE","ODI-<session-id>","",""
   "NAP","IF402-MERCHANT-DETAILS","1021","8500","SYSDATE","ODI-<session-id>","",""

**Data transfer through files**   

For data transfer through files, the source should ideally provide a compressed data file and a control file post-completion of the transfer process.
This process is called "Token aware" - Tokens are files created in the file system to trigger an ETL event. Applications that are token aware can poll a directory for the arrival of a token file. 
The control file holds metadata information about the file transfers. The control file should have an ".info" extension.
There are variety of ways the data file and control file could be related - like one data file and one control file, multi data files and one control file, or multi data files and multi control files.

Take the example of simplest scenario - one compressed data file and one control file. The name of control file should be <data file name>.info.

The structure of control file is comma-delimited::

   cat test.info
   "FILE_NAME","FILE_COUNT","FILE_SIZE","CHECKSUM","CHECKSUM_TYPE"
   "test.gz","20000","43856377","d131dd02c5e6eec4","MD5"

The above control file shows that "test.gz" contains 20,000 files, the file size of "test.gz" is 43856377 bytes and MD5 checksum is d131dd02c5e6eec4.
CKSUM_TYPE could be MD5 or simple UNIX cksum.

The data file names should be unique, both the compressed file and it's content files should be unique. This is to prevent duplicates.
Duplicate files are a common occurrence when files are being received daily/weekly/monthly from another application. 
Sometimes, the previous day’s file has been sent in error. Prior to processing the file, business rules can be executed to ensure the 
correct day’s file will be processed, or in the case of an incorrect file being received, an exception report will be immediately flagged and 
the process will prevent the incorrect file from processing.

On the target side, the control-file should be loaded into the "REC_OWNER.ETL_SOURCE_CONTROL" table:

.. csv-table:: Table REC_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","ETL_BATCH_ID","ROWCOUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF414-INVOICES","test.gz","20000","SYSDATE","ETL Process ID","",""

**Streaming Data transfer - Topics/Queues/Streams**


Reconciliation Framework
------------------------
Combination of ETL tools capabilities and scripting techniques to get counts and balances at the source level when the application 
is providing the data extract. ETL tools are used to read the feed information and
track them by updating and loading metadata tables into data warehouse. At the end of the process, custom
coded reconciliation reports are generated to ensure transparency around the data aggregation process in addition to
transformations that occur during the data movement. 
   
**Counts and Balances** - An example of the value of balancing information is when a program is used to extract data from a source system as input into a target system. 
Business rules are used to balance information such as record counts and sum of amounts on the source file and match the record counts and sum of amounts extracted for input into the target system.

.. csv-table:: Table REC_OWNER.RECON_CONTROL
   :header: "RECON_ID","RECON_FROM","RECON_TO","SOURCE_ID","TARGET_ID","SOURCE_COUNT","TARGET_COUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","STG_CQPR","FINANCIAL_TRANS_EVENT",ETL_BATCH_ID/FILE_NAME,PROCESS_ID,1000,700,"SYSDATE","PROCESS_ID","",""

   
.. csv-table:: Table REC_OWNER.RECON_CONTROL_DETAILS
   :header: "RECON_ID","RECON_TYPE","CATEGORY","KEY","VALUE","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","COUNT","EXCLUSION","REPRESENTMENT",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","EXCLUSION","NON-ONBOARDED",176,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","EXCLUSION","CORRECTS",32,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","BUSINESS-FAILURE","PAN-BLANK",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","BUSINESS-FAILURE","CURRENCY-BLANK",54,"SYSDATE","PROCESS_ID","",""

   
.. csv-table:: Table REC_OWNER.RECON_CONTROL_MASTER
   :header: "RECON_KEY","DESCRIPTION","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto 
   
   "REPRESENTMENT","Representments","SYSDATE","PROCESS_ID","",""
   "NON-ONBOARDED","Merchants not onboarded to Digital","SYSDATE","PROCESS_ID","",""


	
    
