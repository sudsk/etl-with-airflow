Reconciliation
==================================================

.. important::

    Reconciliation increases the integrity levels of information stored within the data store/warehouse. 
	What you will find here are data reconciliation techniques and framework.

Principles
-------------------------
* Reconcile and validate information before it enters the data warehouse
* Validate information before, during, and after Extract, Transform, and Load (ETL) processes
* Verify and reconcile information already in your data warehouse against source systems
* Ensure that information is not lost or erroneously duplicated
* Reconcile report contents against the warehouse
* Perform reasonableness tests to detect potential problems
* Compute statistical tests to feed other controls or determine if expected thresholds are exceeded

Master Data Reconciliation
----------------------------

Reconcile the number of merchants in full load. 

* Source - Number of NAP Merchants - Active, Closed, Suspended 
* Target - Number of NAP Digital onboarded + non-onboarded - Active, Closed, Suspended 
* Source - Number of PAP Merchants -  Active, Closed, Suspended 
* Target - Number of PAP Digital onboarded + non-onboarded - Active, Closed, Suspended 

Reference Data Reconciliation
-------------------------------

Reconcile Currency, MCC, Card Product Type, etc.

			   
Transaction Data Reconciliation
--------------------------------

Reconcile count, balances e.g. sum, etc. from staging till the target layer and capture all business failures, exclusions count.


Source data transfer metadata
------------------------------

Data transfer through tables
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For data transfer through tables, use the below structure:

.. csv-table:: Table ETL_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","TABLE_FILE_NAME","ETL_BATCH_ID","ROW_COUNT","BALANCE_SUM","BALANCE_COLUMN","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF403","STG_CQPR","10001","1996500","672945600","PAYER_AMOUNT","SYSDATE","ODI-<session-id>","",""
   "NAP","IF402","STG_MERCHANT_DETAILS","1021","8500","","SYSDATE","ODI-<session-id>","",""
   
The control record is to be inserted post successful data transfer to the staging table e.g. once data is transferred and committed to STG_CQPR, then 
a record is inserted as shown above into the ETL_SOURCE_CONTROL table. This signifies that the transfer was successful and provides the counts and balances for reconciliation.
The context should exactly match as the staging table populated i.e. SOURCE_SYSTEM, INTERFACE_NAME, TABLE_FILE_NAME, and ETL_BATCH_ID should provide the
necessary context to tie the control record to the staging table. 

Fields Description:
* SOURCE_SYSTEM - NAP, PAP, etc.
* INTERFACE_NAME - IF402, IF403, IF414, IF408, etc.
* TABLE_FILE_NAME - Staging table names or file names e.g. STG_CQPR
* ETL_BATCH_ID - same as the one inserted into the staging table for this particular transfer
* ROW_COUNT - count of records in the staging table or file
* BALANCE_COLUMN - the field used for balances e.g. PAYER_AMOUNT
* BALANCE_SUM - the balance amount, usually the aggregate function should be SUM in all cases
* CREATED_DATE - (audit column) date of creation of this control record
* CREATED_BY - (audit column) process/session-id etc. of the process which inserted this control record
* LAST_UPDATED_DATE - (audit column) left blank as we don't expect this table to be updated
* LAST_UPDATED_BY - (audit column) left blank as we don't expect this table to be updated

Data transfer through files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For data transfer through files, the source should ideally provide a compressed data file and a control file post-completion of the transfer process.
This process is called "Token aware" - Tokens are files created in the file system to trigger an ETL event. Applications that are token aware can poll a directory for the arrival of a token file. 
The control file holds metadata information about the file transfers. The control file should have an ".info" extension.
There are variety of ways the data file and control file could be related - like one data file and one control file, multi data files and one control file, or multi data files and multi control files.

Take the example of simplest scenario - one compressed data file and one control file. The name of control file should be <data file name>.info.

The structure of control file is comma-delimited::

   $ cat test_20180425105233.info
   
   "FILE_NAME","FILE_COUNT","FILE_SIZE","CHECKSUM","CHECKSUM_TYPE"
   "test_20180425105233.gz","20000","43856377","1446861148","CKSUM"

The above control file shows that "test_20180425105233.gz" contains 20,000 files, the file size of "test_20180425105233.gz" is 43856377 bytes and CKSUM checksum is 1446861148.
CKSUM_TYPE could be MD5 or simple UNIX cksum.

The data file names should be unique, both the compressed file and it's content files should be unique. This is to prevent duplicates.
Duplicate files are a common occurrence when files are being received daily/weekly/monthly from another application. 
Sometimes, the previous day’s file has been sent in error. Prior to processing the file, business rules can be executed to ensure the 
correct day’s file will be processed, or in the case of an incorrect file being received, an exception report will be immediately flagged and 
the process will prevent the incorrect file from processing.

On the target side, the control-file should be loaded into the "REC_OWNER.ETL_SOURCE_CONTROL" table:

.. csv-table:: Table ETL_OWNER.ETL_SOURCE_CONTROL
   :header: "SOURCE_SYSTEM","INTERFACE_NAME","TABLE_FILE_NAME","ETL_BATCH_ID","ROW_COUNT","SUM_AMOUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "NAP","IF414","test_20180425105233.gz","105","20000","","SYSDATE","ETL Process ID","",""

Streaming Data transfer
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Kafka Topics, JMS etc.

Reconciliation Framework
------------------------
Combination of ETL tools capabilities and scripting techniques to get counts and balances at the source level when the application 
is providing the data extract. ETL tools are used to read the feed information and
track them by updating and loading metadata tables into data warehouse. At the end of the process, reconciliation reports are generated 
to ensure transparency around the data ingestion process in addition to transformations that occur during the data movement. 
   
**Counts and Balances** - An example of the value of balancing information is when a program is used to extract data from a source system as input into a target system. 
Business rules are used to balance information such as record counts and sum of amounts on the source file and match the record counts and sum of amounts extracted for input into the target system.

.. csv-table:: Table ETL_OWNER.RECON_CONTROL
   :header: "RECON_ID","RECON_FROM","RECON_TO","SOURCE_ID","TARGET_ID","SOURCE_COUNT","TARGET_COUNT","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","STG_CQPR","FINANCIAL_TRANS_EVENT",ETL_BATCH_ID/FILE_NAME,PROCESS_ID,1000,700,"SYSDATE","PROCESS_ID","",""

   
.. csv-table:: Table ETL_OWNER.RECON_CONTROL_DETAILS
   :header: "RECON_ID","RECON_TYPE","CATEGORY","KEY","VALUE","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto   
   
   "1001","COUNT","EXCLUSION","REPRESENTMENT",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","EXCLUSION","NON-ONBOARDED",176,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","EXCLUSION","CORRECTS",32,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","BUSINESS-FAILURE","PAN-BLANK",54,"SYSDATE","PROCESS_ID","",""
   "1001","COUNT","BUSINESS-FAILURE","CURRENCY-BLANK",54,"SYSDATE","PROCESS_ID","",""

   
.. csv-table:: Table ETL_OWNER.RECON_CONTROL_MASTER
   :header: "RECON_KEY","DESCRIPTION","CREATED_DATE","CREATED_BY","LAST_UPDATED_DATE","LAST_UPDATED_BY"
   :widths: auto 
   
   "REPRESENTMENT","Representments","SYSDATE","PROCESS_ID","",""
   "NON-ONBOARDED","Merchants not onboarded to Digital","SYSDATE","PROCESS_ID","",""


	
    
